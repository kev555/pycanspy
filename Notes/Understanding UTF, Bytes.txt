# Characters and Bytes

# When you declare a variable in a programming language, you specify its data type (e.g., integer, string, boolean). 
# This tells the computer how many bits to allocate to the variable and how to interpret the binary sequence stored there.
# "Raw bytes" refer to sequences of bytes that don't have a specific encoding assigned to them. 
# They can be interpreted as text, numbers, or other data types depending on the context and the applied encoding (they have no intrinsic meaning)

# Every "character" in a program is thus stored as bytes.
# Strings can be thought of as an array of characters.
# Python doesn't have a specific single character type, a character is juts a string of length 1
# Java's does have a separate type for single characters, nothing special, mostly just for legacy reasons.

# How these charters are stored / read depends on which character encoding was used, 
# the encoding specifies how the byte or multiped bytes should hold the binary that represents a character
# The same character, say 'Ã„', may be represented by a different arrangement of bytes and maybe multiple bytes per character, 
# depending on which encoding the language / platform uses eg: UTF-8, Latin-1, EBCDIC, etc.
# When you print a string like print(my_string) the byte or bytes at that variable are read, 
# and depending on the character encoding a character image?? is chosen



# UTF:
# Unicode holds a map of all possible characters and labels them with "code points" e.g. "ðŸ˜€" is code point U+1F600
# The UTF code point for every character is the same across UTF-8 versions (UTF-8, UTF-16, UTF-32)
# When sending text each the Hex value of each character's (eg. 1F600 for U+1F600) is encoded into a byte sequence
# Each UTF format will produce a different byte sequence for each character, as each encoding algorithm is different version
# ðŸ˜€ -> U+1F600 -> 1F600 -> encode with UTF-8 -> UTF-8 specific byte sequence -> send the bytes
# The bytes should be sent in order. When receiving they are decoded back to 1F600.

"ðŸ˜€" / U+1F600 byte sequence between versions:
UTF-8 Encoding:	    0xF0 0x9F 0x98 0x80
UTF-16 Encoding:	0xD83D 0xDE00
UTF-32 Encoding:	0x0001F600


# The UTF-8 encoding algorithm:
# Can use either 1,2,3 or 4 bytes (8, 16, 24, or 32 bits) to represent 1 character.
# The first few bits of the first byte are "flags", representing how many bytes were needed to represent the intended character.
# If the first ("leftmost") bit of the first byte is 1, it's saying "this character needed multiple bytes, so an encoding was used to store it across multiple bytes."

# If the first ("leftmost") bit of the first byte is 0, it's saying "this is a single byte character", or to be more verbose: 
# "The following 7 bits were enough to represent this character's unicode point, just use the bit's values directly, no need to decode"

# 7 bits gives the range:
# Binary: 0000000 â†’ 1111111 , aka:
# Decimal: 0 â†’ 127 , aka:
# Hex: 00 â†’ 7F

# So, speaking in decimal, 7 bits = 128 possible code points / characters.
# The Hex values of each Byte lines up perfectly with full printable ASCII character set (7 bit table, 127 values)
# The Unicode code points in this range (0 to 127 / U+00 to U+7F) were set to hold the exact same characters as ASCII
# This makes UTF-8 completely backwards compatible with ASCII.
# If I encode "abc" in UTF-8 and it's read by an ASCII program, which reads the bytes directly, it will still give "abc"

# If the leftmost "flag" is 1, "multiple bytes needed", the algorithm starts reading the bits sequentially to understand:
#   if it's 11xxxxxx - "this is a "starting" byte, continue reading bits for more information"
#   If it's 10xxxxxx - "this is a continuation byte, it's holding data corresponding to other bytes and need to be in a sequence headed by a "starting" byte

# "starting" byte types:
    110x = "this will be a 2 byte character, 1 continuation byte, staring with 10x should follow"
    111x = "this will be a 3 byte character, 2 continuation bytes staring with 10x should follow"
    1111x = "this will be a 4 byte character, 3 continuation bytes staring with 10x should follow"
    11111x - invalid, reserved
    111111x - invalid, reserved
    1111111x - invalid, reserved
    11111111 - invalid, reserved

So:
Bytes that start with 1 are always part of a multi byte character string
Bytes that start with 11 are always starting bytes
Bytes that start with 10 are always continuation bytes
Encoding pattern for every UTF-8 4-byte character is always:
11110xxx 10xxxxxx 10xxxxxx 10xxxxxx
"4 bytes starter", "continuation byte", "continuation byte", "continuation byte"
The bytes 0xC0, 0xC1 and 0xF5 -> 0xFF cannot appear in valid UTF-8.

With these clearly defined rules it allows a debugger to look at a sequence of raw bytes and by looking at the first few bits of each,
to be able to tell if they are where they should be in a particular sequence. This provides a lot of built in error checking.
*It would be possible to free up the first two bits of the continuation bytes and simply rely on their positions relative to the starting bytes to decode, 
but this removes the built in error checking ability aforementioned.

So the total amount of actual data per Bytes used:
Byte Count   Bytes and Bit Pattern                    Available bits
==========   ======================================   ==========================   =======
1            0xxxxxxx  -         -         -                             xxxxxxx    7 bits
2            110xxxxx  10xxxxxx  -         -                        xxxxx_xxxxxx    5+6 = 11 bits
3            1110xxxx  10xxxxxx  10xxxxxx  -                  xxxx_xxxxxx_xxxxxx    4+6+6 = 16 bits
4            11110xxx  10xxxxxx  10xxxxxx  10xxxxxx      xxx_xxxxxx_xxxxxx_xxxxxx   3+6+6+6 = 21 bits

"Available bits" have been organized into the same format here. This is important later for encoding

So.... Remember.... the Unicode code point's hex values are not stored directly, they are "encoded"
eg. 1F600 (for U+1F600) after UTF-8 encoding, in Hex, is: F0 9F 98 80 
If we considered it as a single byte sequence -> hex: F09F9880 - This is a much higher value than 1F600

Unicode tried to choose blocks of the plane to keep similar category of characters together:
"ðŸ˜€" (U+1F600) UTF-8 Encoding: 0xF0 0x9F 0x98 0x80
"ðŸ˜" (U+1F601) UTF-8 Encoding:	0xF0 0x9F 0x98 0x81

But because of the encoding also causes some character blocks that are sequentially next to each other in the unicode plane have vastly different hex values,
as the encoding causes a large range dispersal, if we jump backwards to the previous sequential character in the plane from "ðŸ˜€" (U+1F600):
# "ðŸ—¿"  (U+1F5FF) UTF-8 Encoding: 0xF0 0x9F 0x97 0xBF
# "ðŸ˜€" (U+1F600) UTF-8 Encoding: 0xF0 0x9F 0x98 0x80

How UTF-8 encodes:
Take ðŸ˜€ -> U+1F600 -> 0x1F600. 
Find the range that 1F600 is in:
U+0000 to U+007F: ASCII (1 byte)
U+0080 to U+07FF: Lower section of the BMP (2 bytes)
U+0800 to U+FFFF: Full BMP (3 bytes)
U+10000 to U+10FFFF: Supplementary Planes (4 bytes)

So because 0x1F600 is bigger than 0xFFFF, it needs a 4-byte UTF-8 encoding.
So take the available bit pattern for 4 byte UTF-8, as mentioned above:
Byte Count   Bytes and Bit Pattern                    Available bits
==========   ======================================   ==========================   =======
4            11110xxx  10xxxxxx  10xxxxxx  10xxxxxx     xxx_xxxxxx_xxxxxx_xxxxxx   3+6+6+6 = 21 free bits


"Free space bits" template for 4 byte character: 
11110xxx  10xxxxxx  10xxxxxx  10xxxxxx  (21 free bits)

Now take 1F600 in binary:
11111011000000000 - 17 bits raw binary, no sectioning
As the free space is 21 bits we need to add 5 leading 0's to make it match
0000011111011000000000
Match the format to make it more readable:
000 001111 101100 0000000

Now combine it with the "Free space bits" template's x's, left to right
11110xxx  10xxxxxx  10xxxxxx  10xxxxxx
     000    011111    011000    000000
1111xx00  10011111  10011000  10000000

11110000  10011111  10011000  10000000 is the UTF-8 bite sequence for ðŸ˜€ U+1F600
In hex this is: 0xF09F9880 (0x denotes Hex) 
In Byte-wise hex notation (2 hex digit pairs): 0xF0 0x9F 0x98 0x80

So 0x1F600 has been transformed to 0xF09F9880 (0xF0 0x9F 0x98 0x80 Byte-wise)


Decoding:
Reading UTF-8 binary directly (for ðŸ˜€):
F0 - 11110000 - 1111x   - "this is a 4 byte character"
9F - 10011111 - 10x     - this is a continuation byte
98 - 10011000 - 10x     - this is a continuation byte
80 - 10000000 - 10x     - this is a continuation byte

How UTF-8 decodes F0 9F 98 80:
Remove leading bits (11110 from first byte, and 10 from continuation bytes):
F0 â†’ 11110**000** â†’ 000
9F â†’ 10**011111** â†’ 011111
98 â†’ 10**011000** â†’ 011000
80 â†’ 10**000000** â†’ 000000
==
000 011111 011000 000000
In hex ==
1F600


UTF-8 is very space-efficient, especially for text primarily consisting of ASCII characters, as it uses only 1 byte per character for ASCII. 
It's also widely supported and flexible for storing all Unicode characters, including those from non-Latin scripts, emojis, etc.


# UTF-16:
# Can use either 2 bytes or 4 bytes if needed (16 or 32 bits) to represent 1 character.
# The bytes are considered in pairs (2). 4 Hex values for 1 "pair". 8 Hex values for 2 "pairs".

# A lot of basic text only uses code points / ASCII up to 127, thus only needs 1 byte realistic, 
# but minimum in UTF-16 is 2 bytes, so it's wasteful in a lot of cases.

Characters above U+FFFF need 4 bytes (2 16-bit units)  
These characters are outside the Basic Multilingual Plane (BMP) in Unicode
Basic Multilingual Plane, U+0000 - U+FFFF
Supplementary Multilingual Plane, U+10000 - U+1FFFF

Unlike UTF-8 which uses special bits / bytes (to signal that 2, 3 or 4 bytes were needed instead of 1)
UTF-16 uses special code points (to signal that 4 bytes were needed instead of 1)
In UTF-8, each byte clearly signals whether it's a start byte or a continuation byte.
If a 16-bit code unit is in the high-surrogate range (0xD800â€“0xDBFF), you expect another code unit (a low-surrogate) next.
UTF-16 is slightly less "self-punctuating" than UTF-8.
However it a bit simpler as it's using code points directly

How:
A 16-bit unit between 0xD800 and 0xDBFF (High Surrogate) Signals: "I am the start of a big character!"
A 16-bit unit between 0xDC00 and 0xDFFF (Low Surrogate) Must immediately follow a high surrogate.
Together, the two units represent one real Unicode code point, between U+10000 and U+10FFFF (Supplementary Multilingual Plane)

How many do extra possible values is enabled by using surrogate range combinations:
High surrogates: The range is 0xD800 to 0xDBFF = 2048 values
Low surrogates: The range is 0xDC00 to 0xDFFF = 2048 values
2048 (highÂ surrogateÂ values) Ã— 2048 (lowÂ surrogateÂ values)
= 4,194,304 new possible codeÂ points
âˆ’ 4096 (because 2048 + 2048 are not used singularly now) 
= 4,190,208 new possible codeÂ points 
In hex = ~0x3FFFFF
However Unicode code points only go to U+10FFFF 
(10FFFF = 1,114,111 but actually there are only 1,048,576 Unicode code points in the supplementary range, so i guess about 50k are reserved)

Surrogate pairs could represent higher than 0x10FFFF, but Unicode standard says "No, we stop at 0x10FFFF


Encoding algorithm:
Take the high surrogate and low surrogate,
Remove the 0xD800 and 0xDC00 bases,
Combine their bits,
Add 0x10000 to get the real Unicode code point.

To encode U+1F600 (ðŸ˜€):
Subtract 0x10000 â†’ 0x1F600 - 0x10000 = 0xF600
0xF600 = 1111 0110 0000 0000
Now pad to 20 bits â†’ 0000 1111 0110 0000 0000
Split the 20 bits into two 10-bit pieces
0000111101   1000000000
(high 10)     (low 10)
High 10 bits â†’ 0x3D (hex)
Low 10 bits â†’ 0x600 (hex)

Add bases:
High Surrogate = 0xD800 + 0x3D = 0xD83D
Low Surrogate = 0xDC00 + 0x600 = 0xDE00
UTF-16 stores D83D DE00 (in hex) for ðŸ˜€


# Cool: https://en.wikipedia.org/wiki/Specials_(Unicode_block)
# Another encoding, UTF-32 (previously named UCS-4), uses four bytes (total 32 bits) to encode a single character of the codespace, very wastefully.
# UTF-32 thereby permits a binary representation of every code point (as of year 2024) in the APIs, and software applications.

# Java, Javascript, Windows API, and by many programming environments such as Qt use UFT-16 


UTF-16 Problems?
UTF-16 and UTF-32 are not backwards compatible with ASCII

Since 2 bytes cover the entire Basic Multilingual Plane, U+0000 - U+FFFF,  the vast majority of time UTF-16's double length (4 bytes) is rarely used, 
and thus is rarely tested for. This led to many bugs in software when double length (4 bytes) characters were encountered, including in Windows itself.

UTF-16 uses 16-bit code units, But most communication and storage protocols are defined for 8-bit bytes.
So the order of the bytes may depend on the endianness (byte order) of the computer architecture ie. they get jumbled.
To assist in recognizing the byte order of code units, UTF-16 allows a byte order mark (BOM), a code point with the value U+FEFF, to precede the first actual coded value.
But many times this is missing or ignored, again causing issues

Because of javascript's prevalence, "UTF-16 interoperability will be needed as least as long as the Web is alive": 
https://news.ycombinator.com/item?id=3906590


UTF-8 Problems?
Since UTF-8 is a variable-length encoding, storing text as UTF-8 in a program introduces complexities when manipulating or indexing at the byte level 
str[3] for example might not work if one of the charters is multiple byte. The byte sequence needs to be first decoded into actual characters (overhead?)

Instead of manipulating the raw bytes, you should work with character-based APIs provided by most programming languages (including Python), 
which abstract away the details of encoding and allow you to treat text as characters rather than raw bytes.
In Python, you can use the built-in string functions that work at the character level (not byte level), 
such as len() (which counts characters, not bytes) and slicing (e.g., s[3:5]), which correctly handles multi-byte characters.

However even when using built-in string functions
Example of problem here: https://www.b-list.org/weblog/2017/sep/05/how-python-does-unicode/
On a pre-3.3 â€œnarrowâ€ build of Python 
>>> s = u"\U0001F4A9"
>>> len(s)
2

SO 1 poo emoji returning length of 2, why?
Because narrow build meant that the interpreter build created all characters as 16 bit
But this emoji is outside the BMP. Everything outside the BMP needs 4 bytes.
So it's counting 4 bytes and assuming that's 2 characters = returning 2

u"...":
Because this was before str was unicode (3.0 ->)
U0001F4A9:
Because ðŸ’© (PILE OF POO) is Unicode code point U+1F4A9, which is above U+FFFF.
So you must use "\U" instead of "\u.
And 8 Hex Digits instead of 4 because \U expects exactly 8 hex digits (leading zeros if necessary)

Evolution :
Python 2.x: 
"str" was bytes; 
"unicode" was a separate type (internal encoding = UTF-16 or UTF-32 depending on build).

Python 3.0 (2008):
"str" became Unicode (text).
"bytes" became a separate type for raw data.
But internally, Unicode still used UTF-16 or UTF-32 depending on build (similar to Python 2.x).
UTF-16 narrow build, UTF-32 wide build

Python 3.3 (2012): Introduced PEP 393 ("Flexible String Representation"):
Python stopped committing to a specific internal Unicode encoding.
Strings were stored more efficiently: Latin-1, UCS-2, or UCS-4 chosen on a per string basis, based on the highest character.
Making all stings characters the same size, making manipulating or indexing much easier and more unified, while still saving space for Latin-1 only strings.
UTF-8 was not used internally for str (that would be chaos), but it became the default for source file encoding, filesystem encoding, and I/O encoding.
UTF-8 became the practical, external "default way" of handling text.

So python uses Latin-1 if all are max 8 bits, UCS-2 if all are max 16 bits, UCS-4 if all are 32 bits max.
And always UTF-8 for parsing etc.


# UTF-1 was another format, which encodes all the characters in sequences of bytes of varying length,
# (1 to 5 bytes, each of which contain no control codes) - very problematic




# Byte literals and Bytes objects in python

# Python allows writing bytes directly using "bytes literal" strings with hexadecimal (Base 16) or Octal (base 8) notation
# To denote bases Python uses "\x" for Hex, "\" for Octal. 1 byte in Hex range: 00 -> FF, in Octal: 000 -> 255.

str1 = b'\xE2\x82\xAC'  # NOT a string. A "bytes literal", using python's b'...' notation
print(str1)             # "b'\xe2\x82\xac'" - Prints in python's b'...' notation

# You may assume this is just printing a string, byt it's a real bytes object, test:
print(type(str1))       # "<class 'bytes'>"

# So str1 is a "bytes object", created from a "bytes literal" string of text
# print() does not decode the bytes object automatically as it doesn't know what encoding they are using (utf-8,16,32 etc)

#If you want to decode it to UTF-8 text:
print(str1.decode('utf-8'))     # "â‚¬"

#If you try decode UTF-8 byte sequence with UTF-16 algorithm, will error:
print(str1.decode('utf-16'))    # "UnicodeDecodeError: ... can't decode byte 0xac in position 2"


#######################
????????????????????????????????????????
clean or delete this
????????????????????????????????????????
# Bytes objects just represent binary, they are not specifically for characters. They can translate to characters when encoded
# Strings are specifically for characters and stored in an encoded sequence depending on encoding used
# Encoding can vary by string in python. String specific encoding.


# In Python, byte literals (b'') support only hexadecimal (base 16) and octal (base 8) escape sequences. Here's a breakdown:
# Hexadecimal (Base 16)
# Syntax: \x followed by exactly two hexadecimal digits (0-9, A-F).
# Represents one byte.

# Octal (Base 8)
# Syntax: \ooo where ooo is one to three octal digits (0-7).
# Represents one byte (an 8-bit value).

#bytes literal with hexadecimal values, you prefix the hexadecimal representation of each byte with \x

# # Define a byte string
# byte_string = b"hello world"

# print(byte_string)

# # Convert the byte string to a string using the decode() method
# decoded_string = byte_string.decode("utf-8")

# # Print the decoded string
# print(decoded_string)

# Creates a byte string from a list of integers
# byte_string = bytes([65, 66, 67])
# print(byte_string)  # Output: b'ABC'

# byte_string = b"Hello"  # Creates a byte string directly
# print(byte_string)  # Output: b'Hello'

# blah1 = "b'\x61'" # blah will be a bytes object
# print(blah1)
# blah2 = "b'a'"
# print(blah1, b'\x61')
# b'\x61' # this is a bytes literal

# blah4 = b'a'.decode('UTF-8')
# print("blah4", blah4)
# print(b'DEL' == b'\x04')
# print(b'\xE2\x82\xAC')

# blah3 = b'\x61'.decode('UTF-8') # this is decoding a bytes literal into a UTF-8 string
# print("blah3", blah3)

# You may think when you see b'a' as the output to this that the print is encoding it  , it will print it as a bytes object b'' BUTTTTTTTTT
# print(b'\x1B')  # Escape character - prints b'\x1b'
# print(b'\x2B')  # + character
# print(b'\x80')  # + character
????????????????????????????????????????
clean or delete this
????????????????????????????????????????
######################



Python assumes UTF-8 when parsing text as so decodes as such. 

# Python also has "binary literal" with 0bx notation
binary_number = 0b101010 # a 
# print() will auto decode these:
print(binary_number) # Output: 42

raw_string = r"0b101010"
print(raw_string)


##########
# Also, very confusingly, if Python encounters an ASCII Hex value (1-127) while printing or storing a byte literal, it will auto encode/decode.
print(b'\x61')                  # prints "b'a'"
print(type(b'\x61'))            # prints "<class 'bytes'>" - it's still a bytes object
print(b'\xE2\x82\xAC')          # prints "b'\xe2\x82\xac'" - Because â‚¬ is outside the ASCII range, so just prints with hex
print(type(b'\xE2\x82\xAC'))    # prints "<class 'bytes'>" - it's still a bytes object

print(b'a') # b'a'
print(b'â‚¬') # "Non-ASCII character not allowed in bytes string literal - Pylance"

# This is supposed to make byte literals more compact/readable
# But I found this "feature" EXTREMELY confusing and misleading !!!!!!
##########

##########

# Strings in Python process escape sequences by default (like \n or \xNN):
str_with_escaped_hex = "\x61\x62"    # not a "bytes literal" because no b'...' , a string literal with escaped Hex (\xNN)
print(type(str_with_escaped_hex))    # output: "< class 'str' >" a str as expected
print(str_with_escaped_hex)          # output: "ab" not "\x61\x62"

# Can also escape the escaped Hex:
str_with_escaped_escaped_hex = "\\x61\\x62"
print(type(str_with_escaped_escaped_hex)     # output: "< class 'str' >" a str as expected
print(str_with_escaped_escaped_hex)          # output: "\x61\x62"

print(len(str_with_escaped_escaped_hex), len(str_with_escaped_hex)) # output: 8, 2

When Python parses: str = "\x61\x62" it doesn't store the Hex for every character, just the hex 61 and 62 directly
ie: str['0x61', '0x62'] not: str: ['0x5c', '0x78', '0x36', '0x31', '0x5c', '0x78', '0x36', '0x32']

Python 3.0 (2008): "str" became Unicode (text) - so when printing from either of these it will just print the unicode equivalent
Note: max unicode that can be stored in 8 bits is 0xFF (U+00FF) - "Ã¿" anything past that can't be denoted by an escaped Hex pair

â‚± = U+20B1 = 0x20B1
bytes_as_string = "\x20\xB1"
print(type(bytes_as_string))    
print(bytes_as_string) # output: " Â±" i.e. x20 = empty space in Latin-1 and B1 = Â± in Latin-1 so: " Â±"

print("\x61\x62", type("\x61\x62"))     # output: "ab" < class 'str' >
print(b"\x61\x62", type(b"\x61\x62"))     # output: "\x61\x62" < class 'bytes' >

#############



# You can also use binary escape and store a much higher character:
new_str = "4\321"
print(new_str)
print(len(new_str))
print("size:", sys.getsizeof(new_str)) # 59

# compared to:
new_str2 = "4\\321"
print(new_str2)
print(len(new_str2))
print("size:", sys.getsizeof(new_str2)) # 46



# Deciphering pythons internal encoding using Ã¿ and â‚¬:

# 8 bit tables:
# ISO-8859-1 (also called Latin-1) https://cs.stanford.edu/people/miles/iso8859.html
# Windows-1252 is identical except for the blank (0x80-0x9F) - windows added printable characters in that range

# Unicode Block 1 and 2 (256 Unicode code points) match byte-for-byte with ISO 8859-1 encoding
# Unicode Block 1: Basic Latin (7 bit U+0000 to U+007F) + Unicode Block 2: Latin-1 Supplement (8 bits U+0080 to U+00FF) == ISO 8859-1

# Test for UTF-8:
# In the UTF-8 encoding table:
# U+007F is the maximum Unicode character that is represented by it's identical hex single byte value
# Any Unicode character after 0x7F in the 1 byte range (so 0x7F -> 0xFF) actually requires 2 bytes
# The Unicode hex does not directly map: e.g. U+007F = 0x7F but U+0080 = 0xC2 0x80
# Ã¿ is 0xFF in 8 bit tables, but 0xC5 0xB8 in UTF-8 encoding

# So test for Ã¿:
# str_0xFF = "\xFF"       # not a "bytes literal" because no b'...' - a string literal with escaped Hex (\xNN)
# print(type(str_0xFF))   # output: "< class 'str' >" a str as expected
# print(str_0xFF)         # output: Ã¿

# So we can see it's NOT stored in UTF-8 encoding internally or this wouldn't print anything

# To go a step further let's store different sizes and try to check the memory and try:
# Note getsizeof() gives the entire size of the object in memory, there is no way to 
# check the characters diirectly like in C, as Python is abstracted from that.
# The C underlying internal PyUnicodeObject Structure has multiple fields, something like:
# typedef struct {
#     PyObject_VAR_HEAD
#     PyObject *utf8;            // UTF-8 encoded version of the string
#     Py_UCS4 *unicode;         // UCS-4 (UTF-32) array to store Unicode characters
#     Py_ssize_t length;        // Length of the Unicode string (number of code points)
#     Py_ssize_t hash;          // Cached hash value of the string (for performance)
#     unsigned short state;     // Internal flags or state
#     unsigned short kind;      // Kind of encoding (UTF-8, UCS-2, UCS-4, etc.)
# } PyUnicodeObject;

# so it's not really easy to see much, but let's look:

import sys

# 7 bit characters
test1 = "a"
print("size:", sys.getsizeof(test1)) # size: 42
test2 = "bc"
print("size:", sys.getsizeof(test2)) # size: 43
# 1 byte increments 

# 8 bit characters
test4 = "â‚¬"
print("size:", sys.getsizeof(test4)) # size: 60
test4 = "Æ’Å½"
print("size:", sys.getsizeof(test4)) # size: 62
# 2 byte increments 

# 2 byte characters
test5 = "é‚¬"
print("size:", sys.getsizeof(test5)) # size: 60
test6 = "éƒ•éƒ–"
print("size:", sys.getsizeof(test6)) # size: 62
# 2 byte increments 

# 4 byte characters
test7 = "ðŸ’©"
print("size:", sys.getsizeof(test7)) # size: 64
test8 = "ðŸ’«ðŸ’ª"
print("size:", sys.getsizeof(test8)) # size: 68
# 4 byte increments 

# So judging by the small increases in bytes when  using lower characters, 
# Python seems to be switching between Latin-1, UTF-16, UTF-32 as necessary for each string.



#########################


cv2.waitKey() and the & 0xFF operation:

cv2.waitKey() returns an integer representing the key code of a pressed key. 
This integer might be larger than 8 bits and contain extra information about modifier keys or system-specific flags. 
The & 0xFF operation is used to mask out all but the lowest 8 bits of this integer, which typically represent the standard ASCII code of the key. 
This ensures that the code correctly identifies the pressed key, regardless of any extra information or system-specific variations in the higher bits.


brief example to illustrate how cv2.waitKey() might return a value with extra information in the higher bits, and how & 0xFF extracts the relevant part:

Let's say:
You press the 'q' key while holding down the Ctrl key.
On your specific system, cv2.waitKey() returns the following 32-bit integer (this is just an example; the actual value varies):

00000010 00000000 00000000 01110001  (binary representation)
The lower 8 bits (01110001) represent the ASCII code for 'q' (113 in decimal).
The higher bits (the rest) represent the Ctrl key being pressed (this is system-specific).
Using & 0xFF

If you perform a bitwise AND with 0xFF (which is binary 00000000 0000000 00000000 11111111), you get:

    00000010 00000000 00000000 01110001  (original value)
&   00000000 0000000 00000000 11111111  (0xFF)
=   00000000 0000000 00000000 01110001  (result)
As you can see, the higher bits are all set to 0, and the lower 8 bits (01110001) remain unchanged. This is the ASCII code for 'q'.


In Python:

import cv2
# Simulate cv2.waitKey() returning a value with modifier keys
key_code_with_ctrl = 513  # Example value, represents 'q' + Ctrl

# Extract the lower 8 bits
ascii_code = key_code_with_ctrl & 0xFF

print(f"Key code with Ctrl: {key_code_with_ctrl}")
print(f"ASCII code: {ascii_code}")

if ascii_code == ord('q'):
    print("The 'q' key was pressed (regardless of Ctrl)")
In this example, key_code_with_ctrl is a hypothetical value returned by cv2.waitKey() when 'q' and Ctrl are pressed. 
The & 0xFF operation extracts the ASCII code for 'q', allowing the code to correctly identify the 'q' key press, even if other modifier keys were also pressed.


