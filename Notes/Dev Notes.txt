Packages used so far + their purpose:

To see installed packages:
pip list

pillow:
was installed for using a tkinter to work with images

opencv-python:
cv2 is the official Python module for OpenCV.
It seems the name cv2 is historical and kept for compatibility and not related to the OpenCV version
What actual "version" of OpenCV it's using seems to be difficult to tell.
The documentation for the C++ was horrible, for Python it seems to be non existant.
Possible not all methods are available

Python OpenCV is just a wrapper over the C++ OpenCV API, using bindings 
(e.g. via pybind11)
But Python is dynamically typed, not statically typed like C++, 
So types like cv::Point, cv::Size, and cv::Rect are not needed
The opencv-python library just uses standard built-in Python types where possible, such as tuples - https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences

For the cv::Mat type (matrix), it uses the ndarray (n-dimensional array) type from the NumPy package
cv2.imread() reads the image as a cv::Mat in C++ then it converts that cv::Mat into a numpy.ndarray before handing it to the Python code.

equivelent of - 		cv::Mat image = cv::imread("photo.jpg") 
would sort of be - 	numpy::ndarray image =  cv2.imread("photo.jpg")


Draw Rectangle Example:

rectangle() is part of the drawing functions, under the module imgproc
(very poorly documented, as everthing else)
C++: https://docs.opencv.org/4.x/d6/d6e/group__imgproc__draw.html#ga07d2f74cadcf8e305e810ce8eed13bc9
Python: 
https://docs.opencv.org/4.x/dc/da5/tutorial_py_drawing_functions.html

"The function cv::rectangle draws a rectangle outline or a filled rectangle whose two opposite corners are pt1 and pt2"
- so each obviously holds a set of corordinates for each opposing corner's of an intended rectangle.

In C++:
// create a blank image
cv::Mat image = cv::Mat::zeros(400, 400, CV_8UC3);
// store each opposing corner cordinates
cv::Point topLeft(50, 50);
cv::Point bottomRight(300, 300);
// need a variable for rgb, 3 values, OpenCV C++ uses a Scalar for this
cv::Scalar all_blue(255, 0, 0); 
// draw a on the image with rectangle() (the 2 is just border thickness in pixels)
cv::rectangle(image, topLeft, bottomRight, all_blue, 2);
// show it
cv::imshow("Rectangle", image);
// keep window open
cv::waitKey(0)

- Clearly defined types: Point and Size can tell you exactly what the numbers mean
- With types the compiler can check for errors before runtime

In Python:
# create a blank image
image = numpy.ones((400, 600, 3), dtype=numpy.uint8) * 255
# store each opposing corner cordinates in tuples:
topLeft = (50, 50)
bottomRight = (300, 300)
# simple tuple to store rgb values
blue = (255, 0, 0)
# draw a on the image with rectangle() (the 2 is just border thickness in pixels)
cv2.rectangle(image, topLeft, bottomRight, blue, 2)
# show it
cv2.imshow("Rectangle", image)
# keep window open
cv2.waitKey(0)

Or if you really wanted:
image = numpy.ones((400, 600, 3), dtype=numpy.uint8) * 255
cv2.rectangle(image, (50, 50), (300, 300), (255, 0, 0), 2)
cv2.imshow("Rectangle", image)
cv2.waitKey(0)

- Much shorter code, easier to write
- But no type checking —> wrong values can cause bugs


NOTES ON cv::Scalar AND cv::Vec
================================

cv::Scalar
----------
- cv::Scalar is a typedef for cv::Vec<double, 4>.
- It's used for representing color values and other 4-element tuples in OpenCV.
- Adds convenience functionality on top of cv::Vec, e.g.:
    cv::Scalar(255) → automatically fills all channels with 255 (makes white).
- Common in OpenCV drawing functions like cv::rectangle, cv::circle, etc.

From OpenCV docs:
"Being derived from Vec<_Tp, 4>, Scalar_ and Scalar can be used just as typical 4-element vectors. In addition, they can be converted to/from CvScalar. The type Scalar is widely used in OpenCV to pass pixel values."

cv::Vec (basically a fixed size "vector" implementation)
-------
- A template class: cv::Vec<T, N> holds N elements of type T.
- Unlike std::vector, cv::Vec is fixed-size and stack-allocated.
- Very lightweight and fast; ideal for pixel data and small vector math.
- Examples:
    - cv::Vec3b: 3 unsigned bytes (e.g. BGR pixel)
    - cv::Vec4f: 4 floats
    - cv::Vec4d: 4 doubles (same as cv::Scalar)


cv::Vec 
- Basically a fixed size "vector" implementation
- A template class: cv::Vec<T, N> holds N elements of type T.
- Because it's fixed size the vectors are allocated on stack, unlike std::vector or even cv::Mat where elements are dynamically allocated in the heap. This makes them very fast.

https://docs.opencv.org/3.4/d6/dcf/classcv_1_1Vec.html

So cv::Scalar is just a Vec4d from this list (with sie added functionality):
https://docs.opencv.org/2.4/modules/core/doc/basic_structures.html#vec


Note on Heap Usage
------------------
If you store pointers inside a cv::Vec, the data they point to lives on the heap.
This breaks the key benefit of using cv::Vec (everything stack-allocated).
The cv::Vec object itself however will still be on the stack unless you explicitly allocate it with `new`.


Summary Table
-------------
Type            | Memory Location       | Fixed Size? | Notes
--------------|------------------------- |-------------|-----------------------------------------------------
cv::Vec         | Stack (by default)     | Yes         | Fast, stack-allocated, ideal for color/math values
cv::Scalar      | Stack (by default)    | Yes         | Just a Vec<double, 4> with extras
std::vector    | Stack + Heap            | No          | Object on stack, elements on heap, resizable
cv::Vec with ptrs | Stack (ptrs) + Heap  | Yes         | Loses its main advantage if used with dynamic data


Seems there is no fully separate Python API reference - the Python module mimics the C++ one, but with:
Python-style method names. No headers, types, or pointers. Sometimes different parameter handling (e.g., default values)

Since OpenCV doesn't maintain a comprehensive Python API reference like C++ does, the most reliable method is in-code introspection:
help(cv2.VideoCapture)
or:
cap = cv2.VideoCapture(0)
print(dir(cap))
help(cap.read)
This will show you all methods and their docstrings.

The python opencv documentation project is abandoned: https://opencv-python-tutroals.readthedocs.io/en/latest/



pywin32:
Initially installed to create a Windows service for running a separate background process.
Not currently needed, as the application now uses a subprocess launched directly by the GUI.
To use:
import win32service
import win32serviceutil
https://github.com/mhammond/pywin32/blob/main/win32/Lib/win32serviceutil.py

When you start a Windows service using win32serviceutil, Windows launches it as a separate process.
This process is managed by the Windows Service Control Manager (SCM).
It runs independently of the process that started it (i.e. the GUI).


Multithreading vs Multiprocessing:
Multithreading - no new process, uses same memory
Multiprocessing - new process, uses new memory

Multiple Threads run in the same Process memory as each other. 
Processes have separate memory (you will see seperate processes in taskmanager or ps)

Concurrency vs Parallelism:
For concurrency python offers Threading (Lib/threading.py) or Coroutines (asyncio) (these use 1 instance of the Python interpreter)
For parallelism python offers multiprocessing (this uses multiple instances of the Python interpreter)

Concurrency:
Threading: Python Threads using a single interpreter instance between them. 
The Global Interpreter Lock (GIL) allows only 1 Thread to execute Python bytecode in the interpreter at a time.
Sounds to me not much different to javascript's single thread with event loop.... 
Coroutines: Yes, in python they have "Asyncio" which utilizes a single-threaded event loop to handle concurrency.

Note: Threads in python use OS-level (native) threads (as oppsed to "user-level threads" aka "green threads" eg goroutines in Go which are managed by the Go runtime).

However:
"Threads can sometimes outperform async for CPU-bound tasks if the thread runs C extensions or native code that releases the GIL (e.g., NumPy, OpenCV).
While the GIL is released, other threads can run, allowing real parallelism."

But wouldnt this possibly cause race conditions?
"When a C extension (like NumPy or OpenCV) releases the GIL, it does so only while executing code that is already thread-safe, 
or where the Python interpreter state doesn't need to be touched. So race conditions are avoided because:
The C code is usually carefully written to manage its own memory/thread safety.
Python’s own memory and interpreter state are protected by reacquiring the GIL before interacting with them again."

Interesting. So for CPU heavy tasks sometimes Threading (with OS threads only?) can actuly use parallelism, and thus be more performant (OS threads only?)
But there is the added overhed of setting up and managing threads.
"The theoretical benefit of coroutines over threads is you can create many more of them, because they are cheap to start and use less memory"

- Multiprocessing is often faster for large, separate tasks, while multithreading can be faster for small, interconnected tasks
- The threading module uses threads, the multiprocessing module uses processes. 
- Harder to share objects between processes with multiprocessing
- Since threads use the same memory, global interpreter lock makes sure two threads will not write to the same memory at the same time.
- Spawning Processes is slower than spawning Threads, but opviously multi processing is faster:
    https://medium.com/contentsquare-engineering-blog/multithreading-vs-multiprocessing-in-python-ece023ad55a
    https://eli.thegreenplace.net/2012/01/16/python-parallelizing-cpu-bound-tasks-with-multiprocessing/


Very Interesting:
""
As far as I can see, basically the entire scientific python stack will continue to rely heavily on threads for the foreseeable future. 
Huge chunks of that stack do release the GIL so threads do operate in parallel - the common example is numpy itself, 
but there's ton's of stuff both for doing CPU intensive work (like machine learning with scikit-learn) and doing I/O on large data files (e.g. GBs of weather data in NetCDF files).
In general this needs repeating a lot more: a data/science applications is really huge part of the python community. 
We don't care (mostly) about webservers, async or otherwise. We get performance by writing critical elements in Cython/C/Fortran (and hopefully Rust in the near future). 
The GIL is not a big deal because native extensions release it.
That said, most people probably shouldn't be writing threads directly but using a schedular/task queue to do it form them (such as Dask or Joblib)
""


Parallelism: (Multiprocessing, Subprocess)

Processes spawned via "multiprocessing.Process" can use easy communication methods provided by "multiprocessing.Manager",
wheras a process started via "subprocess" cannot.

"The subprocess module lets you run and control other programs. Anything you can start with the command line on the computer, 
can be run and controlled with this module. Use this to integrate external programs into your Python code."

"The multiprocessing module lets you divide tasks written in python over multiple processes to help improve performance. 
It provides an API very similar to the threading module; it provides methods to share data across the processes it creates, 
and makes the task of managing multiple processes to run Python code (much) easier. 
In other words, multiprocessing lets you take advantage of multiple processes to get your tasks done faster by executing code in parallel."

"The subprocess module lets you run and control other programs."

Does it use a thread to create a new process asynchronously, and the thread continues to communicate with the process???

"The new process is run as a fork of the current process, which is then replaced by the child process via an execv or execve system call. 
No threads are involved nor would they help in creating the new process any faster. Communication is mostly handled by pipes (so, the OS). 
If you want to use threads, you can, or you could use asyncio. Your choice"

So I will not use subprocess anymore.
Or... I could have the GUI and the camera management as seperate processes
the GUI process would use Unix sockets or "named pipes" (on windows) to message the camera managment process
the camera managment process could also take direct messages from: a remote device (via a socket), a local webapp (again via a socket)


####
####

Additional Notes on, scope stuff in python vs JS:
Scope in Python and JavaScript:
In Python, variables are local to a function unless declared as global or nonlocal.
In JavaScript, var, let, and const have different scope behaviors, but all of them respect their enclosing scope when functions are defined within them.

Global and Nonlocal in Python:
global: Used to modify global variables from inside a function.
nonlocal: Used to modify variables from an enclosing (but not global) scope.

Classes and Functions in Python:
In Python, functions can be defined inside classes, and methods within classes typically access the class attributes through self.
Unlike JavaScript, Python doesn't allow functions to be directly added as properties to an object without using a class.

Lambda Functions:
Lambda functions in Python are a way to define anonymous functions, similar to JavaScript's anonymous functions. They can access variables from their enclosing scope.

JavaScript Object Methods:
JavaScript allows functions to be directly added to objects (e.g., obj.myfunc), where this refers to the object itself.

Functions Inside Objects in Python:
Python doesn't support defining functions directly inside dictionaries in the same way JavaScript does, because Python doesn't automatically bind the function to the object's context (self in classes).
You can use lambda in Python to define functions that access variables from their parent scope, but using nonlocal would not work in this case for nested dictionary definitions.

Scope Chain Behavior:
this in JavaScript refers to the object the function is part of and behaves differently depending on where the function is called.
Python doesn't have an equivalent to this and instead uses self inside class methods to refer to the instance of the class.



########
########

Understanding sockets terminology and standards (as the similar terms are confusing so full explanation):

A Berkeley (aka BSD) socket API:
For Network communication: defines the "Internet domain sockets" definition. Uses IPv4/IPv6. Only usable over networks that utilize those protocols e.g. the Internet!
For IPC communication: defines the "Unix domain sockets" definition. Does NOT use network protocols like IPv4/IPv6. Only for unix-like systems (Linux, macOS, etc.) 
 - Modern Berkeley Network sockets (aka. "Internet domain sockets") and IPC sockets (aka. "Unix domain sockets") adhere to POSIX standards ()"POSIX-compliant sockets")
 - Both use the term "sockets", which can be confusing. Also sometimes referred to as "Berkeley" "BSD" sockets. 

Winsock aka Windows Sockets API:
For Network communication, defines the term "sockets", but used specificilly to describe network sockets. Also IPv4/IPv6....
For IPC communication, Windows does not use e.g, "Windows domain sockets". Instead, it uses "Named Pipes"
 - Winsock API is actually based on Berkeley Sockets API, but is NOT POSIX-compliant
 - Around 2017, Microsoft added limited support for Unix domain sockets in Windows (AF_UNIX), but it's not widely used or feature-complete.

*POSIX compliance means the API behaves in a predictable and portable way across POSIX-compliant systems (e.g., Linux, BSD, macOS).

When network sockets from either API use IPv4 or IPv6, they are denoted as AF_INET and AF_INET6 sockets respectfully (PF_INET and PF_INET6 in some legacy code).
Linux IPC uses Unix domain sockets , they are designated as AF_UNIX (or PF_UNIX in legacy code)
Windows IPC use Named Pipes so don't have a designated address family like AF_UNIX. Just specify with a pipe name: \\.\pipe\

When network sockets from either API use TCP they are denoted by socket type constant "SOCK_STREAM". For UDP it's "SOCK_DGRAM".

* IPC methods ("Unix domain sockets" or Windows "Named Pipes") can also be denoted "SOCK_STREAM" or "SOCK_DGRAM", 
  as here it's refering to the communication "semantics" of the socket (stream vs. datagram), not the underlying protocol (TCP, UDP)


Demo of minimal code necessary to create an IPv4 TCP socket in Python, Linux (Berkeley Sockets API), and Windows (Winsock API).
Python:
import socket
s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

Linux / Unix (Berkeley Sockets API):
#include <sys/socket.h>  // For socket creation and communication
#include <netinet/in.h>  // For defining the socket address structure and constants
int sockfd;
sockfd = socket(AF_INET, SOCK_STREAM, 0);  // Create TCP socket

Windows (Winsock API):
#include <winsock2.h>
#pragma comment(lib, "ws2_32.lib")  // Link against Winsock library
WSADATA wsaData;
SOCKET sockfd;
WSAStartup(MAKEWORD(2, 2), &wsaData);  // Initialize Winsock
sockfd = socket(AF_INET, SOCK_STREAM, 0);  // Create TCP socket




#####
#####

cv2.waitKey() and the & 0xFF operation:

cv2.waitKey() returns an integer representing the key code of a pressed key. 
This integer might be larger than 8 bits and contain extra information about modifier keys or system-specific flags. 
The & 0xFF operation is used to mask out all but the lowest 8 bits of this integer, which typically represent the standard ASCII code of the key. 
This ensures that the code correctly identifies the pressed key, regardless of any extra information or system-specific variations in the higher bits.


brief example to illustrate how cv2.waitKey() might return a value with extra information in the higher bits, and how & 0xFF extracts the relevant part:

Let's say:
You press the 'q' key while holding down the Ctrl key.
On your specific system, cv2.waitKey() returns the following 32-bit integer (this is just an example; the actual value varies):

00000010 00000000 00000000 01110001  (binary representation)
The lower 8 bits (01110001) represent the ASCII code for 'q' (113 in decimal).
The higher bits (the rest) represent the Ctrl key being pressed (this is system-specific).
Using & 0xFF

If you perform a bitwise AND with 0xFF (which is binary 00000000 0000000 00000000 11111111), you get:

    00000010 00000000 00000000 01110001  (original value)
&   00000000 0000000 00000000 11111111  (0xFF)
=   00000000 0000000 00000000 01110001  (result)
As you can see, the higher bits are all set to 0, and the lower 8 bits (01110001) remain unchanged. This is the ASCII code for 'q'.


In Python:

import cv2
# Simulate cv2.waitKey() returning a value with modifier keys
key_code_with_ctrl = 513  # Example value, represents 'q' + Ctrl

# Extract the lower 8 bits
ascii_code = key_code_with_ctrl & 0xFF

print(f"Key code with Ctrl: {key_code_with_ctrl}")
print(f"ASCII code: {ascii_code}")

if ascii_code == ord('q'):
    print("The 'q' key was pressed (regardless of Ctrl)")
In this example, key_code_with_ctrl is a hypothetical value returned by cv2.waitKey() when 'q' and Ctrl are pressed. 
The & 0xFF operation extracts the ASCII code for 'q', allowing the code to correctly identify the 'q' key press, even if other modifier keys were also pressed.

